---
title: "chapter4.Rmd"
author: "Valeria Caras"
date: "11/20/2019"
output: html_document
---
## Exercise 4
```{r }
## Data overview
library(MASS)
data("Boston")
str(Boston)
summary(Boston)
#variables distribution
p <- pairs(Boston)
p
#calculate the correlation matrix and round it
cor_matrix<-cor(Boston) 
#print the correlation matrix
cor_matrix
#visualize the correlation matrix
library(corrplot)
library(tidyverse)
corrplot(cor_matrix, method="circle")
corrplot(cor_matrix, method="circle", type = "upper",cl.pos = "b", tl.pos = "d",tl.cex = 0.6)%>%round()
```

This data has 506 obs. of  14 variables which contain information about different indicators of Boston city as per capita crime rate, index of accessibility to radial highways and etc.
In the fist plot we seen the distibution of variables. It looks like only rm has a normal distribution while crime, chas, lstat are shifted to the left, age to the right and etc.
In the second plot positive correlations are displayed in blue and negative correlations in red color. Color intensity and the size of the circle are proportional to the correlation coefficients. For example, the is strong positive correlation between such variables as crime and rad and tax, between indus and nox, age and tax. Negative correlations between dis and nox,age and dis, medv and lstat. 

```{r }
## Scaling and factor variable

#center and standardize variables
boston_scaled <- scale (Boston)
#summaries of the scaled variables
summary(boston_scaled) 
#class of the boston_scaled object
class(boston_scaled)
#change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
#create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
#create a categorical variable 'crime'
vector <- c("low","med_low","med_high","high")
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=vector)
#look at the table of the new factor crime
table(crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
#add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

## Train and test sets
#number of rows in the Boston dataset 
n <- nrow(boston_scaled)
#choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
#create train set
train <- boston_scaled[ind,]
#create test set 
test <- boston_scaled[-ind,]
#save the correct classes from test data
correct_classes <- c(test$crime)
#remove the crime variable from test data
test <- dplyr::select(test, -crime)

str(train)
str(test)
```

Here we scale the data which is an operation when we subtract the column means from the corresponding columns and divide the difference with standard deviation. It helps us to have normal distribution of variables later used in clastering.
When we create a factor variable'crim' and use the quantiles as the break points to the variable.Later we divide the dataset to train and test sets, so that 80% of the data belongs to the train set.

```{r}
# Linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
#print the lda.fit object
lda.fit

#the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

#target classes as numeric
classes <- as.numeric(train$crime)
#plot the lda results
plot(lda.fit, dimen = 2,col = classes,pch = classes)
lda.arrows(lda.fit, myscale = 2)
```


We have 4 clasters, so train data was devided in 25% 4 times with crime as target variable. In the plot we see that three clasters are overlapping while the 4rd one in quite far for them. We aslo see that 
such variables as zn, nox, rad, ptratio have a big impact on the model

```{r}
# Predictors
#create train set
train <- boston_scaled[ind,]
#create test set 
test <- boston_scaled[-ind,]
#save the correct classes from test data
correct_classes <- c(test$crime)
#remove the crime variable from test data
test <- dplyr::select(test, -crime)
#predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

#target classes as numeric
classes <- as.numeric(correct_classes)
#plot the lda results
plot(lda.fit, dimen = 2,col = classes,pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

In the table we see the relation between the correct classes and the predicted ones. The classifier did not predict the crime rates correctly since predicted numbers are higher than correct. I also tried to vizualize the LDA results for crime in test data and we see that everything is in a mess.

```{r}
## distances
library(MASS)
data('Boston')
#scale the data
boston_scaled <- scale (Boston)
#euclidean distance matrix
dist_eu <- (boston_scaled)
#look at the summary of the distances
summary(dist_eu)
#manhattan distance matrix
dist_man <- dist(boston_scaled, method = "manhattan")
#look at the summary of the distances
summary(dist_man)
# k-means clustering
km <-kmeans(boston_scaled, centers = 1)
km <-kmeans(boston_scaled, centers = 3)
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col=km$cluster)
#determine K
set.seed(123)
library(ggplot2)
#determine the number of clusters
k_max <- 10
#calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
#visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
#k-means clustering
km <-kmeans(boston_scaled, centers = 2)

```

Based on the calculated distance meajure the k-classtering was maded. On the graph we see a significant change in point 2 - so the optimal number of clastters is 2.
On the plot we see the vizualization for 2 clasters - which part of data where belongs.


```{r}
## 3D plot
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

library(plotly)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')

```

